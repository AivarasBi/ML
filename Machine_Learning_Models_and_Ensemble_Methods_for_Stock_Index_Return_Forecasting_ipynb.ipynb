{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EZJx8rr5Ugj"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1x5MIEc5Me6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import (MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer)\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.linear_model import (LinearRegression, ElasticNet, ElasticNetCV, SGDRegressor)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import (Input, Dense, Dropout, Flatten, LSTM, GRU, TimeDistributed,LayerNormalization\n",
        "                                     , MultiHeadAttention, GlobalAveragePooling1D, Bidirectional, Layer)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber, Loss\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.models import Sequential as KerasSequential\n",
        "from keras.layers import Dense as KerasDense, Dropout as KerasDropout, BatchNormalization, LeakyReLU\n",
        "from keras.optimizers import Adam as KerasAdam\n",
        "from keras.callbacks import EarlyStopping as KerasEarlyStopping, ReduceLROnPlateau as KerasReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.regularizers import l2\n",
        "from keras.losses import Huber as KerasHuber\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    LSTM,\n",
        "    GRU,\n",
        "    LayerNormalization,\n",
        "    MultiHeadAttention,\n",
        "    Layer\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber, Loss\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ReduceLROnPlateau\n",
        ")\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import warnings\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "\n",
        "import yfinance as yf\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score\n",
        ")\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n"
      ],
      "metadata": {
        "id": "NjUxj8SSpcsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b-6fQ_c5bIF"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwrT1Xy7uPPc"
      },
      "outputs": [],
      "source": [
        "df_GSPC = pd.read_excel(\"df_GSPC.xlsx\", index_col=\"Date\")\n",
        "df_NDX = pd.read_excel(\"df_NDX.xlsx\", index_col=\"Date\")\n",
        "df_DJI = pd.read_excel(\"df_DJI.xlsx\", index_col=\"Date\")\n",
        "df_FTSE = pd.read_excel(\"df_FTSE.xlsx\", index_col=\"Date\")\n",
        "df_GDAXI = pd.read_excel(\"df_GDAXI.xlsx\", index_col=\"Date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbEeIKJuFyId"
      },
      "source": [
        "# Other functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AojH-IZZF06D"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, time_step=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset) - time_step):\n",
        "        a = dataset[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, mae, r2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3HW8tA-y6An"
      },
      "outputs": [],
      "source": [
        "def generate_signal(row, treshhold):\n",
        "    if abs(row['Prediction'] / row['Close_yesterday'] -1) <= treshhold:\n",
        "        return 'no_signal'\n",
        "    elif row['Prediction'] > row['Close_yesterday']:\n",
        "        return 'buy'\n",
        "    elif row['Prediction'] < row['Close_yesterday']:\n",
        "        return 'sell'\n",
        "    else:\n",
        "        return 'no_signal'\n",
        "\n",
        "def calculate_trades(df, initial_capital, upper_level_loss=1.025, lower_level_loss=0.975, upper_level_win=1.2, lower_level_win=0.8):\n",
        "    capital = initial_capital\n",
        "    current_position = None\n",
        "    current_shares = 0\n",
        "    buy_price = 0\n",
        "    taxes = 0\n",
        "    taxes_rate = 0.001\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        row = df.iloc[i]\n",
        "        signal = row['signal']\n",
        "        open_price = row['Open']\n",
        "        high_price = row['High']\n",
        "        low_price = row['Low']\n",
        "        close_price = row['Close']\n",
        "\n",
        "        transaction = 'Not_traded'\n",
        "\n",
        "        if current_position is None:\n",
        "            if signal == 'buy':\n",
        "                current_shares = capital // open_price\n",
        "                taxes = taxes_rate * current_shares * open_price\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital = capital - taxes\n",
        "                buy_price = open_price\n",
        "                current_position = 'buy'\n",
        "                transaction = 'Open_Buy'\n",
        "            elif signal == 'sell':\n",
        "                current_shares = capital // open_price\n",
        "                taxes = taxes_rate * current_shares * open_price\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital = capital - taxes\n",
        "                buy_price = open_price\n",
        "                current_position = 'sell'\n",
        "                transaction = 'Open_Short'\n",
        "        elif current_position == 'buy':\n",
        "            if signal == 'sell':\n",
        "                pnl = (open_price - buy_price) * current_shares\n",
        "                df.at[row.name, 'PnL'] = pnl\n",
        "                capital += pnl\n",
        "                taxes = taxes_rate * current_shares * open_price + capital // open_price * open_price * taxes_rate\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital = capital - taxes\n",
        "                transaction = 'Close_Buy_Signal/Open_Short'\n",
        "                current_shares = capital // open_price\n",
        "                buy_price = open_price\n",
        "                current_position = 'sell'\n",
        "            elif high_price >= buy_price * upper_level_win:\n",
        "                pnl = (max(buy_price * upper_level_win,open_price) - buy_price) * current_shares\n",
        "                df.at[row.name, 'PnL'] = pnl\n",
        "                taxes = taxes_rate * current_shares * max(buy_price * upper_level_win,open_price)\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital += pnl\n",
        "                capital = capital - taxes\n",
        "                transaction = 'Close_Buy_reached_High'\n",
        "                current_position = None\n",
        "            elif low_price <= buy_price * lower_level_loss:\n",
        "                pnl = (min(buy_price * lower_level_loss,open_price) - buy_price) * current_shares\n",
        "                df.at[row.name, 'PnL'] = pnl\n",
        "                capital += pnl\n",
        "                taxes = taxes_rate * current_shares * min(buy_price * lower_level_loss,open_price)\n",
        "                capital = capital - taxes\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                transaction = 'Close_Buy_reached_Low'\n",
        "                current_position = None\n",
        "            else:\n",
        "                df.at[row.name, 'PnL'] = 0\n",
        "                transaction = 'Held_position'\n",
        "\n",
        "        elif current_position == 'sell':\n",
        "            if signal == 'buy':\n",
        "                pnl = (buy_price - open_price) * current_shares\n",
        "                df.at[row.name, 'PnL'] = pnl\n",
        "                taxes = taxes_rate * current_shares * open_price + capital // open_price * open_price * taxes_rate\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital += pnl\n",
        "                capital = capital - taxes\n",
        "                transaction = 'Close_Short_Signal/Open_Buy'\n",
        "                current_shares = capital // open_price\n",
        "                buy_price = open_price\n",
        "                current_position = 'buy'\n",
        "            elif low_price <= buy_price * lower_level_win:\n",
        "                pnl = (buy_price - min(buy_price * lower_level_win,open_price)) * current_shares\n",
        "                capital += pnl\n",
        "                df.at[row.name, 'PnL'] = pnl\n",
        "                taxes = taxes_rate * current_shares * min(buy_price * lower_level_win,open_price)\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital = capital - taxes\n",
        "                transaction = 'Close_Short_reached_Low'\n",
        "                current_position = None\n",
        "            elif high_price >= buy_price * upper_level_loss:\n",
        "                pnl = (buy_price - max(buy_price * upper_level_loss,open_price)) * current_shares\n",
        "                capital += pnl\n",
        "                df.at[row.name, 'PnL'] = pnl\n",
        "                taxes = taxes_rate * current_shares * max(buy_price * upper_level_loss,open_price)\n",
        "                df.at[row.name, 'taxes'] = taxes\n",
        "                capital = capital - taxes\n",
        "                transaction = 'Close_Short_reached_High'\n",
        "                current_position = None\n",
        "            else:\n",
        "                df.at[row.name, 'PnL'] = 0\n",
        "                transaction = 'Held_position'\n",
        "\n",
        "        df.at[row.name, 'shares'] = current_shares\n",
        "        df.at[row.name, 'capital'] = capital\n",
        "        df.at[row.name, 'position'] = current_position\n",
        "        df.at[row.name, 'transaction'] = transaction\n",
        "        if current_position == 'buy':\n",
        "            unrealized_capital = capital + (close_price - buy_price) * current_shares\n",
        "        elif current_position == 'sell':\n",
        "            unrealized_capital = capital + (buy_price - close_price) * current_shares\n",
        "        else:\n",
        "            unrealized_capital = capital\n",
        "\n",
        "        df.at[row.name, 'unrealized_capital'] = unrealized_capital\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_return_and_drawdown(df, initial_cash=100000, tax=0.01):\n",
        "\n",
        "    buy_price = df.iloc[0]['Open']\n",
        "    buy_price_with_tax = buy_price * (1 + tax)\n",
        "\n",
        "    shares = int(initial_cash // buy_price_with_tax)\n",
        "    invested_amount = shares * buy_price_with_tax\n",
        "\n",
        "    leftover_cash = initial_cash - invested_amount\n",
        "\n",
        "    value_low = df['Low'] * shares + leftover_cash\n",
        "    cumulative_max = value_low.cummax()\n",
        "    drawdowns = (value_low - cumulative_max) / cumulative_max\n",
        "    max_drawdown = drawdowns.min()\n",
        "\n",
        "    sell_price = df.iloc[-1]['Close']\n",
        "    sell_price_after_tax = sell_price * (1 - tax)\n",
        "    final_value = shares * sell_price_after_tax + leftover_cash\n",
        "\n",
        "    total_return = (final_value - initial_cash) / initial_cash\n",
        "\n",
        "    return total_return, max_drawdown\n",
        "\n",
        "def financials(data, column_name, name, algorithm, treshhold,plotting = 0):\n",
        "\n",
        "  df = data.copy()\n",
        "  df.rename(columns={column_name: 'Prediction'}, inplace=True)\n",
        "\n",
        "  mse_fin, mae_fin, r2_fin = calculate_metrics(df[\"Close\"], df[\"Prediction\"])\n",
        "\n",
        "  df[\"Change\"] = df[\"Close\"] / df[\"Close_yesterday\"] - 1\n",
        "  df[\"pred_change\"] = df[\"Prediction\"] / df[\"Close_yesterday\"] - 1\n",
        "  df[\"direction_correct\"] = np.where((df[\"Change\"] * df[\"pred_change\"]) >= 0,1,0).astype(int)\n",
        "  directional_accuracy = df[\"direction_correct\"].mean() * 100\n",
        "\n",
        "  df['signal'] = df.apply(generate_signal, args=(treshhold,), axis=1)\n",
        "  df = df.dropna()\n",
        "  initial_capital = 100000.0\n",
        "  df['shares'] = 0.0\n",
        "  df['PnL'] = 0.0\n",
        "  df['taxes'] = 0.0\n",
        "  df['capital'] = initial_capital\n",
        "  df['unrealized_capital'] = initial_capital\n",
        "  df['position'] = None\n",
        "  df['transaction'] = 'None'\n",
        "\n",
        "\n",
        "  financials = calculate_trades(df, initial_capital, upper_level_loss=1.05, lower_level_loss=0.95, upper_level_win=1.1, lower_level_win=0.9)\n",
        "  financials.index = pd.to_datetime(financials.index)\n",
        "  cumulative_max = financials['unrealized_capital'].cummax()\n",
        "  drawdown = (financials['unrealized_capital'] - cumulative_max) / cumulative_max\n",
        "  max_drawdown = drawdown.min()\n",
        "\n",
        "  total_return = (financials['unrealized_capital'].iloc[-1] - initial_capital) / initial_capital\n",
        "\n",
        "  trading_days = len(financials)\n",
        "  annualized_return = (1 + total_return) ** (252 / trading_days) - 1\n",
        "\n",
        "  calmar_ratio = annualized_return / abs(max_drawdown)\n",
        "\n",
        "  trades_qy = (financials[\"taxes\"] != 0).sum()\n",
        "\n",
        "  total_return_threshold, max_drawdown_threshold = calculate_return_and_drawdown(financials, initial_cash=initial_capital, tax=0.001)\n",
        "\n",
        "  metrics = {\n",
        "      'Name': name,\n",
        "      'Algorithm' : algorithm,\n",
        "      'MSE': mse_fin,\n",
        "      'MAE': mae_fin,\n",
        "      'R_squered': r2_fin,\n",
        "      'Max Drawdown': [f'{max_drawdown * 100:.2f}%'],\n",
        "      'Total Return': [total_return],\n",
        "      'Annualized Return': annualized_return * 100,\n",
        "      'Calmar Ratio': [calmar_ratio],\n",
        "      'Max Drawdown Threshold': [f'{max_drawdown_threshold * 100:.2f}%'],\n",
        "      'Total Return Threshold': [f'{total_return_threshold * 100:.2f}%'],\n",
        "      'trades': [trades_qy],\n",
        "      'directional_accuracy': [directional_accuracy]\n",
        "  }\n",
        "\n",
        "  metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "  if plotting == 1:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(financials.index, financials['capital'], label='Capital')\n",
        "    plt.plot(financials.index, financials['unrealized_capital'], label='Unrealized Capital', linestyle='--')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Capital')\n",
        "    plt.legend()\n",
        "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(False)\n",
        "    plt.savefig(\"financials.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.plot(financials.index, financials[\"Close\"], label='True Price')\n",
        "    plt.plot(financials.index, financials[\"Prediction\"], label='Predicted Price')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Stock Price')\n",
        "    plt.title(rf\"{algorithm} - Final Test Set Predictions - {name}\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "  return metrics_df, financials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSSX-_-_5sRz"
      },
      "outputs": [],
      "source": [
        "def prepare_data(df, start_date_train, end_date_train, start_date_test, end_date_test, time_steps):\n",
        "\n",
        "  df.index = pd.to_datetime(df.index)\n",
        "\n",
        "  df['Close_yesterday'] = df['Close'].shift(1)\n",
        "\n",
        "  df[\"Change\"] = df[\"Close\"] / df[\"Close_yesterday\"] - 1\n",
        "\n",
        "  df = df.dropna(subset=['Change'])\n",
        "\n",
        "  start_date_train = pd.to_datetime(start_date_train)\n",
        "  end_date_train = pd.to_datetime(end_date_train)\n",
        "  start_date_test = pd.to_datetime(start_date_test)\n",
        "  end_date_test = pd.to_datetime(end_date_test)\n",
        "\n",
        "  data_train = df[(df.index <= end_date_train) & (df.index >= start_date_train)].copy()\n",
        "\n",
        "  start_idx = df.index.get_indexer([start_date_test], method='bfill')[0]\n",
        "  start_with_padding_idx = max(0, start_idx - time_steps)\n",
        "  start_with_padding_date = df.index[start_with_padding_idx]\n",
        "\n",
        "  data_test = df[(df.index >= start_with_padding_date) & (df.index <= end_date_test)].copy()\n",
        "\n",
        "  data_final_test = df[(df.index >= start_date_test) & (df.index <= end_date_test)].copy()\n",
        "\n",
        "  scaled_data = data_train[['Change']].values\n",
        "  scaled_data_fin = data_test[['Change']].values\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaled_data = scaler.fit_transform(scaled_data)\n",
        "  scaled_data_fin = scaler.transform(scaled_data_fin)\n",
        "\n",
        "  X_train, y_train = create_dataset(scaled_data, time_steps)\n",
        "\n",
        "  X_test, y_test_fin = create_dataset(scaled_data_fin, time_steps)\n",
        "\n",
        "  X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "  return data_final_test, X_train, y_train, X_test, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_ewNdmDiMne"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y57vq9hExtX"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msJJlf9auRCY"
      },
      "outputs": [],
      "source": [
        "def LSTM_Model(df, name, start_date_train = '2000-01-01', end_date_train = '2010-01-01', start_date_test = '2021-01-01',\n",
        "               end_date_test = '2023-01-01', units_layer1=128, units_layer2=64, dropout_rate=0.1,\n",
        "               learning_rate=1e-4, epochs=4, batch_size=16, time_steps=5, plotting = 0):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  data_test_final, X_train, y_train, X_test_fin, scaler = prepare_data(df, start_date_train, end_date_train, start_date_test, end_date_test, time_steps)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(time_steps, 1)))\n",
        "  model.add(LSTM(units_layer1, return_sequences=True))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(LSTM(units_layer2))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(), metrics=['mae'])\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "            validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "  y_pred_lstm_fin = model.predict(X_test_fin)\n",
        "\n",
        "  y_pred_lstm_fin = scaler.inverse_transform(y_pred_lstm_fin.reshape(-1, 1))\n",
        "\n",
        "  data_test_final['pred_change'] = y_pred_lstm_fin[:, 0]\n",
        "\n",
        "  data_test_final[\"direction_correct\"] = np.where((data_test_final[\"Change\"] * data_test_final[\"pred_change\"]) >= 0,1,0).astype(int)\n",
        "  directional_accuracy = data_test_final[\"direction_correct\"].mean() * 100\n",
        "\n",
        "  data_test_final[\"Prediction\"] = (data_test_final[\"pred_change\"] + 1) * (data_test_final[\"Close_yesterday\"])\n",
        "\n",
        "  mse_fin, mae_fin, r2_fin = calculate_metrics(data_test_final[\"Close\"], data_test_final[\"Prediction\"])\n",
        "\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  results_dict = {\n",
        "      'name': name,\n",
        "      'start_date_train' : start_date_train,\n",
        "      'end_date_train' : end_date_train,\n",
        "      'start_date_test' : start_date_test,\n",
        "      'end_date_test' : end_date_test,\n",
        "      'time_steps' : time_steps,\n",
        "      'units_layer1': units_layer1,\n",
        "      'units_layer2': units_layer2,\n",
        "      'dropout_rate': dropout_rate,\n",
        "      'learning_rate': learning_rate,\n",
        "      'epochs': epochs,\n",
        "      'batch_size': batch_size,\n",
        "      'mse': mse_fin,\n",
        "      'mae': mae_fin,\n",
        "      'r2': r2_fin,\n",
        "      'directional_accuracy': directional_accuracy,\n",
        "      'training_time_sec': training_time\n",
        "      }\n",
        "\n",
        "  if plotting == 1:\n",
        "      plt.figure(figsize=(14, 5))\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Close\"], label='True Price')\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Prediction\"], label='Predicted Price')\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Stock Price')\n",
        "      plt.title(rf\"LSTM - Final Test Set Predictions - {name}\")\n",
        "      plt.legend()\n",
        "      plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "      plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(14, 5))\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Change\"], label='True Change')\n",
        "      plt.plot(data_test_final.index, data_test_final[\"pred_change\"], label='Predicted Change')\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Stock Price Change')\n",
        "      plt.title(rf\"LSTM - Final Test Set Predictions - {name}\")\n",
        "      plt.legend()\n",
        "      plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "      plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.show()\n",
        "\n",
        "  return data_test_final, results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AArr4n_tFAF2"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlY1uGYDFF9W"
      },
      "outputs": [],
      "source": [
        "def GRU_Model(df, name, start_date_train = '2000-01-01', end_date_train = '2010-01-01', start_date_test = '2021-01-01',\n",
        "               end_date_test = '2023-01-01', units_layer1=128, units_layer2=64, dropout_rate=0.1,\n",
        "               learning_rate=1e-4, epochs=4, batch_size=16, time_steps=5, plotting = 0):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  data_test_final, X_train, y_train, X_test_fin, scaler = prepare_data(df, start_date_train, end_date_train, start_date_test, end_date_test, time_steps)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(time_steps, 1)))\n",
        "  model.add(GRU(units_layer1, return_sequences=True))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(GRU(units_layer2))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(), metrics=['mae'])\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "            validation_split=0.2, verbose=1, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "  y_pred_lstm_fin = model.predict(X_test_fin)\n",
        "\n",
        "  y_pred_lstm_fin = scaler.inverse_transform(y_pred_lstm_fin.reshape(-1, 1))\n",
        "\n",
        "  data_test_final['pred_change'] = y_pred_lstm_fin[:, 0]\n",
        "\n",
        "  data_test_final[\"direction_correct\"] = np.where((data_test_final[\"Change\"] * data_test_final[\"pred_change\"]) >= 0,1,0).astype(int)\n",
        "  directional_accuracy = data_test_final[\"direction_correct\"].mean() * 100\n",
        "\n",
        "  data_test_final[\"Prediction\"] = (data_test_final[\"pred_change\"] + 1) * (data_test_final[\"Close_yesterday\"])\n",
        "\n",
        "  mse_fin, mae_fin, r2_fin = calculate_metrics(data_test_final[\"Close\"], data_test_final[\"Prediction\"])\n",
        "\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  results_dict = {\n",
        "      'name': name,\n",
        "      'start_date_train' : start_date_train,\n",
        "      'end_date_train' : end_date_train,\n",
        "      'start_date_test' : start_date_test,\n",
        "      'end_date_test' : end_date_test,\n",
        "      'time_steps' : time_steps,\n",
        "      'units_layer1': units_layer1,\n",
        "      'units_layer2': units_layer2,\n",
        "      'dropout_rate': dropout_rate,\n",
        "      'learning_rate': learning_rate,\n",
        "      'epochs': epochs,\n",
        "      'batch_size': batch_size,\n",
        "      'mse': mse_fin,\n",
        "      'mae': mae_fin,\n",
        "      'r2': r2_fin,\n",
        "      'directional_accuracy': directional_accuracy,\n",
        "      'training_time_sec': training_time\n",
        "      }\n",
        "\n",
        "  if plotting == 1:\n",
        "      plt.figure(figsize=(14, 5))\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Close\"], label='True Price')\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Prediction\"], label='Predicted Price')\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Stock Price')\n",
        "      plt.title(rf\"GRU - Final Test Set Predictions - {name}\")\n",
        "      plt.legend()\n",
        "      plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "      plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(14, 5))\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Change\"], label='True Change')\n",
        "      plt.plot(data_test_final.index, data_test_final[\"pred_change\"], label='Predicted Change')\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Stock Price Change')\n",
        "      plt.title(rf\"GRU - Final Test Set Predictions - {name}\")\n",
        "      plt.legend()\n",
        "      plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "      plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "      plt.xticks(rotation=45)\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "  return data_test_final, results_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvbWHa0l_9W-"
      },
      "source": [
        "## DMLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVu6A-1HHnHc"
      },
      "outputs": [],
      "source": [
        "def DMLP_Model(df, name, start_date_train = '2000-01-01', end_date_train = '2010-01-01', start_date_test = '2021-01-01',\n",
        "               end_date_test = '2023-01-01', units_layer1=128, units_layer2=64, dropout_rate=0.1,\n",
        "               learning_rate=1e-4, epochs=4, batch_size=16, time_steps=5, plotting = 0):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  data_test_final, X_train, y_train, X_test_fin, scaler = prepare_data(df, start_date_train, end_date_train, start_date_test, end_date_test, time_steps)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(time_steps, 1)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units_layer1, activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(units_layer2, activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(1))\n",
        "\n",
        "  model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(), metrics=['mae'])\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "  model.fit(X_train, y_train,\n",
        "      epochs=epochs, batch_size=batch_size,\n",
        "      validation_split=0.2, verbose=1,\n",
        "      callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "  y_pred_dmlp_fin = model.predict(X_test_fin)\n",
        "  y_pred_dmlp_fin = scaler.inverse_transform(y_pred_dmlp_fin.reshape(-1, 1))\n",
        "\n",
        "  data_test_final['pred_change'] = y_pred_dmlp_fin[:, 0]\n",
        "\n",
        "  data_test_final[\"direction_correct\"] = np.where((data_test_final[\"Change\"] * data_test_final[\"pred_change\"]) >= 0, 1, 0).astype(int)\n",
        "  directional_accuracy = data_test_final[\"direction_correct\"].mean() * 100\n",
        "\n",
        "  data_test_final[\"Prediction\"] = (data_test_final[\"pred_change\"] + 1) * (data_test_final[\"Close_yesterday\"])\n",
        "\n",
        "  mse_fin, mae_fin, r2_fin = calculate_metrics(data_test_final[\"Close\"], data_test_final[\"Prediction\"])\n",
        "\n",
        "  end_time = time.time()\n",
        "  training_time = end_time - start_time\n",
        "\n",
        "  results_dict = {\n",
        "      'name': name,\n",
        "      'start_date_train' : start_date_train,\n",
        "      'end_date_train' : end_date_train,\n",
        "      'start_date_test' : start_date_test,\n",
        "      'end_date_test' : end_date_test,\n",
        "      'time_steps' : time_steps,\n",
        "      'units_layer1': units_layer1,\n",
        "      'units_layer2': units_layer2,\n",
        "      'dropout_rate': dropout_rate,\n",
        "      'learning_rate': learning_rate,\n",
        "      'epochs': epochs,\n",
        "      'batch_size': batch_size,\n",
        "      'mse': mse_fin,\n",
        "      'mae': mae_fin,\n",
        "      'r2': r2_fin,\n",
        "      'directional_accuracy': directional_accuracy,\n",
        "      'training_time_sec': training_time\n",
        "  }\n",
        "\n",
        "  if plotting == 1:\n",
        "      plt.figure(figsize=(14, 5))\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Close\"], label='True Price')\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Prediction\"], label='Predicted Price')\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Stock Price')\n",
        "      plt.title(rf\"DMLP - Final Test Set Predictions - {name}\")\n",
        "      plt.legend()\n",
        "      plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "      plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.show()\n",
        "\n",
        "      plt.figure(figsize=(14, 5))\n",
        "      plt.plot(data_test_final.index, data_test_final[\"Change\"], label='True Change')\n",
        "      plt.plot(data_test_final.index, data_test_final[\"pred_change\"], label='Predicted Change')\n",
        "      plt.xlabel('Time')\n",
        "      plt.ylabel('Stock Price Change')\n",
        "      plt.title(rf\"DMLP - Final Test Set Predictions - {name}\")\n",
        "      plt.legend()\n",
        "      plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "      plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "      plt.xticks(rotation=45)\n",
        "      plt.show()\n",
        "\n",
        "  return data_test_final, results_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSYrGRhaA4uc"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugmFKoDqcf19"
      },
      "outputs": [],
      "source": [
        "def XGB_Model(df, name, start_date_train='2000-01-01', end_date_train='2010-01-01',\n",
        "              start_date_test='2021-01-01', end_date_test='2023-01-01',\n",
        "              n_estimators=300, max_depth=4, learning_rate=0.05,\n",
        "              subsample=0.9, colsample_bytree=0.9,\n",
        "              time_steps=5, plotting=0, random_state=42):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    data_test_final, X_train, y_train, X_test_fin, scaler = prepare_data(df, start_date_train, end_date_train, start_date_test, end_date_test, time_steps)\n",
        "\n",
        "    X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "    X_test_flat  = X_test_fin.reshape((X_test_fin.shape[0], -1))\n",
        "\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        verbosity=0)\n",
        "\n",
        "    model.fit(X_train_flat, y_train)\n",
        "\n",
        "    y_pred_xgb_scaled = model.predict(X_test_flat).reshape(-1, 1)\n",
        "    y_pred_xgb_fin = scaler.inverse_transform(y_pred_xgb_scaled)\n",
        "\n",
        "    data_test_final['pred_change'] = y_pred_xgb_fin[:, 0]\n",
        "\n",
        "    data_test_final[\"direction_correct\"] = np.where((data_test_final[\"Change\"] * data_test_final[\"pred_change\"]) >= 0, 1, 0).astype(int)\n",
        "    directional_accuracy = data_test_final[\"direction_correct\"].mean() * 100\n",
        "\n",
        "    data_test_final[\"Prediction\"] = (data_test_final[\"pred_change\"] + 1) * (data_test_final[\"Close_yesterday\"])\n",
        "\n",
        "    mse_fin, mae_fin, r2_fin = calculate_metrics(data_test_final[\"Close\"], data_test_final[\"Prediction\"])\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    results_dict = {\n",
        "        'name': name,\n",
        "        'start_date_train': start_date_train,\n",
        "        'end_date_train': end_date_train,\n",
        "        'start_date_test': start_date_test,\n",
        "        'end_date_test': end_date_test,\n",
        "        'time_steps': time_steps,\n",
        "        'n_estimators': n_estimators,\n",
        "        'max_depth': max_depth,\n",
        "        'learning_rate': learning_rate,\n",
        "        'subsample': subsample,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'random_state': random_state,\n",
        "        'mse': mse_fin,\n",
        "        'mae': mae_fin,\n",
        "        'r2': r2_fin,\n",
        "        'directional_accuracy': directional_accuracy,\n",
        "        'training_time_sec': training_time\n",
        "    }\n",
        "\n",
        "    if plotting == 1:\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(data_test_final.index, data_test_final[\"Close\"], label='True Price')\n",
        "        plt.plot(data_test_final.index, data_test_final[\"Prediction\"], label='Predicted Price')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Stock Price')\n",
        "        plt.title(rf\"XGBoost - Final Test Set Predictions - {name}\")\n",
        "        plt.legend()\n",
        "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(data_test_final.index, data_test_final[\"Change\"], label='True Change')\n",
        "        plt.plot(data_test_final.index, data_test_final[\"pred_change\"], label='Predicted Change')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Stock Price Change')\n",
        "        plt.title(rf\"XGBoost - Final Test Set Predictions - {name}\")\n",
        "        plt.legend()\n",
        "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "\n",
        "    return data_test_final, results_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiSDincdQIv"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5jAi8f8dSKm"
      },
      "outputs": [],
      "source": [
        "def Transformer_Model(\n",
        "    df, name,\n",
        "    start_date_train='2000-01-01', end_date_train='2010-01-01',\n",
        "    start_date_test='2021-01-01', end_date_test='2023-01-01',\n",
        "    d_model=64, num_heads=4, ff_dim=128, num_layers=2,\n",
        "    dropout_rate=0.1, learning_rate=1e-4,\n",
        "    epochs=10, batch_size=32, time_steps=20, plotting=0):\n",
        "\n",
        "    class PositionalEmbedding(Layer):\n",
        "        def __init__(self, maxlen, d_model, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.maxlen = maxlen\n",
        "            self.d_model = d_model\n",
        "            self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=d_model)\n",
        "\n",
        "        def call(self, x):\n",
        "            seq_len = tf.shape(x)[1]\n",
        "            positions = tf.range(start=0, limit=seq_len, delta=1)\n",
        "            pos_encoding = self.pos_emb(positions)\n",
        "            return x + pos_encoding\n",
        "\n",
        "    class TransformerEncoderBlock(Layer):\n",
        "        def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
        "            self.ffn = tf.keras.Sequential([\n",
        "                Dense(ff_dim, activation='relu'),\n",
        "                Dropout(dropout_rate),\n",
        "                Dense(d_model),\n",
        "            ])\n",
        "            self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "            self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "            self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "        def call(self, x, training=False):\n",
        "            attn_output = self.mha(x, x, training=training)\n",
        "            attn_output = self.dropout(attn_output, training=training)\n",
        "            out1 = self.norm1(x + attn_output)\n",
        "\n",
        "            ffn_output = self.ffn(out1, training=training)\n",
        "            ffn_output = self.dropout(ffn_output, training=training)\n",
        "            out2 = self.norm2(out1 + ffn_output)\n",
        "            return out2\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    data_test_final, X_train, y_train, X_test_fin, scaler = prepare_data(\n",
        "        df, start_date_train, end_date_train, start_date_test, end_date_test, time_steps\n",
        "    )\n",
        "    assert len(X_train.shape) == 3 and X_train.shape[-1] == 1\n",
        "\n",
        "\n",
        "    inp = Input(shape=(time_steps, 1))\n",
        "    x = Dense(d_model)(inp)\n",
        "    x = PositionalEmbedding(maxlen=time_steps, d_model=d_model)(x)\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerEncoderBlock(d_model, num_heads, ff_dim, dropout_rate)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    out = Dense(1)(x)\n",
        "\n",
        "    model = Model(inp, out)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(), metrics=['mae'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        validation_split=0.2, verbose=1,\n",
        "        callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "\n",
        "    y_pred_scaled = model.predict(X_test_fin)\n",
        "    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
        "    data_test_final['pred_change'] = y_pred[:, 0]\n",
        "\n",
        "    data_test_final[\"direction_correct\"] = np.where((data_test_final[\"Change\"] * data_test_final[\"pred_change\"]) >= 0, 1, 0).astype(int)\n",
        "    directional_accuracy = data_test_final[\"direction_correct\"].mean() * 100\n",
        "\n",
        "    data_test_final[\"Prediction\"] = (data_test_final[\"pred_change\"] + 1) * (data_test_final[\"Close_yesterday\"])\n",
        "\n",
        "    mse_fin, mae_fin, r2_fin = calculate_metrics(data_test_final[\"Close\"], data_test_final[\"Prediction\"])\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    results_dict = {\n",
        "        'name': name,\n",
        "        'start_date_train': start_date_train,\n",
        "        'end_date_train': end_date_train,\n",
        "        'start_date_test': start_date_test,\n",
        "        'end_date_test': end_date_test,\n",
        "        'time_steps': time_steps,\n",
        "        'd_model': d_model,\n",
        "        'num_heads': num_heads,\n",
        "        'ff_dim': ff_dim,\n",
        "        'num_layers': num_layers,\n",
        "        'dropout_rate': dropout_rate,\n",
        "        'learning_rate': learning_rate,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'mse': mse_fin,\n",
        "        'mae': mae_fin,\n",
        "        'r2': r2_fin,\n",
        "        'directional_accuracy': directional_accuracy,\n",
        "        'training_time_sec': training_time\n",
        "    }\n",
        "\n",
        "    if plotting == 1:\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(data_test_final.index, data_test_final[\"Close\"], label='True Price')\n",
        "        plt.plot(data_test_final.index, data_test_final[\"Prediction\"], label='Predicted Price')\n",
        "        plt.xlabel('Time'); plt.ylabel('Stock Price')\n",
        "        plt.title(rf\"Transformer - Final Test Set Predictions - {name}\")\n",
        "        plt.legend()\n",
        "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(data_test_final.index, data_test_final[\"Change\"], label='True Change')\n",
        "        plt.plot(data_test_final.index, data_test_final[\"pred_change\"], label='Predicted Change')\n",
        "        plt.xlabel('Time'); plt.ylabel('Stock Price Change')\n",
        "        plt.title(rf\"Transformer - Final Test Set Predictions - {name}\")\n",
        "        plt.legend()\n",
        "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "\n",
        "    return data_test_final, results_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1NpA7_Bhgrl"
      },
      "source": [
        "# Train best and prepare for ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6-RE4eDC046"
      },
      "outputs": [],
      "source": [
        "LSTM_GSPC_train, results_dict = LSTM_Model(df_GSPC, \"LSTM_GSPC_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_GSPC_test, results_dict = LSTM_Model(df_GSPC, \"LSTM_GSPC_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_FTSE_train, results_dict = LSTM_Model(df_FTSE, \"LSTM_FTSE_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_FTSE_test, results_dict = LSTM_Model(df_FTSE, \"LSTM_FTSE_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_GDAXI_train, results_dict = LSTM_Model(df_GDAXI, \"LSTM_GDAXI_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_GDAXI_test, results_dict = LSTM_Model(df_GDAXI, \"LSTM_GDAXI_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_DJI_train, results_dict = LSTM_Model(df_DJI, \"LSTM_DJI_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_DJI_test, results_dict = LSTM_Model(df_DJI, \"LSTM_DJI_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_NDX_train, results_dict = LSTM_Model(df_NDX, \"LSTM_NDX_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "LSTM_NDX_test, results_dict = LSTM_Model(df_NDX, \"LSTM_NDX_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_GSPC_train, results_dict = GRU_Model(df_GSPC, \"GRU_GSPC_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_GSPC_test, results_dict = GRU_Model(df_GSPC, \"GRU_GSPC_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_FTSE_train, results_dict = GRU_Model(df_FTSE, \"GRU_FTSE_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_FTSE_test, results_dict = GRU_Model(df_FTSE, \"GRU_FTSE_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = v, plotting = 0)\n",
        "\n",
        "GRU_NDX_train, results_dict = GRU_Model(df_NDX, \"GRU_NDX_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_NDX_test, results_dict = GRU_Model(df_NDX, \"GRU_NDX_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_GDAXI_train, results_dict = GRU_Model(df_GDAXI, \"GRU_GDAXI_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_GDAXI_test, results_dict = GRU_Model(df_GDAXI, \"GRU_GDAXI_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_DJI_train, results_dict = GRU_Model(df_DJI, \"GRU_DJI_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "GRU_DJI_test, results_dict = GRU_Model(df_DJI, \"GRU_DJI_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_DJI_train, results_dict = DMLP_Model(df_DJI, \"DMLP_DJI_train\",'2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_DJI_test, results_dict = DMLP_Model(df_DJI, \"DMLP_DJI_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_GSPC_train, results_dict = DMLP_Model(df_GSPC, \"DMLP_GSPC_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_GSPC_test, results_dict = DMLP_Model(df_GSPC, \"DMLP_GSPC_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_FTSE_train, results_dict = DMLP_Model(df_FTSE, \"DMLP_FTSE_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_FTSE_test, results_dict = DMLP_Model(df_FTSE, \"DMLP_FTSE_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_NDX_train, results_dict = DMLP_Model(df_NDX, \"DMLP_NDX_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_NDX_test, results_dict = DMLP_Model(df_NDX, \"DMLP_NDX_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_GDAXI_train, results_dict = DMLP_Model(df_GDAXI, \"DMLP_GDAXI_train\", '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "DMLP_GDAXI_test, results_dict = DMLP_Model(df_GDAXI, \"DMLP_GDAXI_test\", '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                           units_layer1 = 256, units_layer2 = 128,\n",
        "                                           dropout_rate = 0.3, learning_rate = 0.001, epochs = 500,\n",
        "                                           batch_size = 64, time_steps = 30, plotting = 0)\n",
        "\n",
        "\n",
        "Transformer_GDAXI_train, results_dict = Transformer_Model(df_GDAXI, \"Transformer_GDAXI_train\",\n",
        "                                                          '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 500, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_GDAXI_test, results_dict = Transformer_Model(df_GDAXI, \"Transformer_GDAXI_test\",\n",
        "                                                          '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 300, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_GSPC_train, results_dict = Transformer_Model(df_GSPC, \"Transformer_GSPC_train\",\n",
        "                                                          '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 500, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_GSPC_test, results_dict = Transformer_Model(df_GSPC, \"Transformer_GSPC_test\",\n",
        "                                                          '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 300, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_FTSE_train, results_dict = Transformer_Model(df_FTSE, \"Transformer_FTSE_train\",\n",
        "                                                          '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 500, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_FTSE_test, results_dict = Transformer_Model(df_FTSE, \"Transformer_FTSE_test\",\n",
        "                                                          '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 300, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_NDX_train, results_dict = Transformer_Model(df_NDX, \"Transformer_NDX_train\",\n",
        "                                                          '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 500, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_NDX_test, results_dict = Transformer_Model(df_NDX, \"Transformer_NDX_test\",\n",
        "                                                         '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 300, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_DJI_train, results_dict = Transformer_Model(df_DJI, \"Transformer_DJI_train\",\n",
        "                                                          '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 500, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "Transformer_DJI_test, results_dict = Transformer_Model(df_DJI, \"Transformer_DJI_test\",\n",
        "                                                          '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                                          d_model = 64, num_heads = 4, ff_dim = 256, num_layers = 3,\n",
        "                                                          dropout_rate = 0.3, learning_rate = 0.0001,\n",
        "                                                          epochs = 300, batch_size = 64,\n",
        "                                                          time_steps = 30, plotting = 0)\n",
        "\n",
        "\n",
        "XGB_DJI_train, results_dict  = XGB_Model(df_DJI, \"XGB_DJI_train\",\n",
        "                                         '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_DJI_test, results_dict  = XGB_Model(df_DJI, \"XGB_DJI_test\",\n",
        "                                         '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_GSPC_train, results_dict  = XGB_Model(df_GSPC, \"XGB_GSPC_train\",\n",
        "                                         '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_GSPC_test, results_dict  = XGB_Model(df_GSPC, \"XGB_GSPC_test\",\n",
        "                                         '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_FTSE_train, results_dict  = XGB_Model(df_FTSE, \"XGB_FTSE_train\",\n",
        "                                         '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_FTSE_test, results_dict  = XGB_Model(df_FTSE, \"XGB_FTSE_test\",\n",
        "                                         '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_NDX_train, results_dict  = XGB_Model(df_NDX, \"XGB_NDX_train\",\n",
        "                                         '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_NDX_test, results_dict  = XGB_Model(df_NDX, \"XGB_NDX_test\",\n",
        "                                         '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_GDAXI_train, results_dict  = XGB_Model(df_GDAXI, \"XGB_GDAXI_train\",\n",
        "                                         '2000-01-03', '2021-01-01', '2021-01-01', '2024-01-01',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "XGB_GDAXI_test, results_dict  = XGB_Model(df_GDAXI, \"XGB_GDAXI_test\",\n",
        "                                         '2000-01-03', '2024-01-01', '2024-01-01', '2025-11-30',\n",
        "                                         n_estimators = 300, max_depth = 3, learning_rate = 0.005,\n",
        "                                         subsample = 0.7 , colsample_bytree = 1,\n",
        "                                         time_steps = 30, plotting = 1, random_state = 7)\n",
        "\n",
        "LSTM_GSPC_train = LSTM_GSPC_train.rename(columns={'Prediction': 'LSTM_GSPC'})\n",
        "LSTM_GSPC_test = LSTM_GSPC_test.rename(columns={'Prediction': 'LSTM_GSPC'})\n",
        "\n",
        "LSTM_FTSE_train = LSTM_FTSE_train.rename(columns={'Prediction': 'LSTM_FTSE'})\n",
        "LSTM_FTSE_test = LSTM_FTSE_test.rename(columns={'Prediction': 'LSTM_FTSE'})\n",
        "\n",
        "LSTM_GDAXI_train = LSTM_GDAXI_train.rename(columns={'Prediction': 'LSTM_GDAXI'})\n",
        "LSTM_GDAXI_test = LSTM_GDAXI_test.rename(columns={'Prediction': 'LSTM_GDAXI'})\n",
        "\n",
        "LSTM_DJI_train = LSTM_DJI_train.rename(columns={'Prediction': 'LSTM_DJI'})\n",
        "LSTM_DJI_test = LSTM_DJI_test.rename(columns={'Prediction': 'LSTM_DJI'})\n",
        "\n",
        "LSTM_NDX_train = LSTM_NDX_train.rename(columns={'Prediction': 'LSTM_NDX'})\n",
        "LSTM_NDX_test = LSTM_NDX_test.rename(columns={'Prediction': 'LSTM_NDX'})\n",
        "\n",
        "GRU_GSPC_train = GRU_GSPC_train.rename(columns={'Prediction': 'GRU_GSPC'})\n",
        "GRU_GSPC_test = GRU_GSPC_test.rename(columns={'Prediction': 'GRU_GSPC'})\n",
        "\n",
        "GRU_FTSE_train = GRU_FTSE_train.rename(columns={'Prediction': 'GRU_FTSE'})\n",
        "GRU_FTSE_test = GRU_FTSE_test.rename(columns={'Prediction': 'GRU_FTSE'})\n",
        "\n",
        "GRU_NDX_train = GRU_NDX_train.rename(columns={'Prediction': 'GRU_NDX'})\n",
        "GRU_NDX_test = GRU_NDX_test.rename(columns={'Prediction': 'GRU_NDX'})\n",
        "\n",
        "GRU_GDAXI_train = GRU_GDAXI_train.rename(columns={'Prediction': 'GRU_GDAXI'})\n",
        "GRU_GDAXI_test = GRU_GDAXI_test.rename(columns={'Prediction': 'GRU_GDAXI'})\n",
        "\n",
        "GRU_DJI_train = GRU_DJI_train.rename(columns={'Prediction': 'GRU_DJI'})\n",
        "GRU_DJI_test = GRU_DJI_test.rename(columns={'Prediction': 'GRU_DJI'})\n",
        "\n",
        "DMLP_DJI_train = DMLP_DJI_train.rename(columns={'Prediction': 'DMLP_DJI'})\n",
        "DMLP_DJI_test = DMLP_DJI_test.rename(columns={'Prediction': 'DMLP_DJI'})\n",
        "\n",
        "DMLP_GSPC_train = DMLP_GSPC_train.rename(columns={'Prediction': 'DMLP_GSPC'})\n",
        "DMLP_GSPC_test = DMLP_GSPC_test.rename(columns={'Prediction': 'DMLP_GSPC'})\n",
        "\n",
        "DMLP_FTSE_train = DMLP_FTSE_train.rename(columns={'Prediction': 'DMLP_FTSE'})\n",
        "DMLP_FTSE_test = DMLP_FTSE_test.rename(columns={'Prediction': 'DMLP_FTSE'})\n",
        "\n",
        "DMLP_NDX_train = DMLP_NDX_train.rename(columns={'Prediction': 'DMLP_NDX'})\n",
        "DMLP_NDX_test = DMLP_NDX_test.rename(columns={'Prediction': 'DMLP_NDX'})\n",
        "\n",
        "DMLP_GDAXI_train = DMLP_GDAXI_train.rename(columns={'Prediction': 'DMLP_GDAXI'})\n",
        "DMLP_GDAXI_test = DMLP_GDAXI_test.rename(columns={'Prediction': 'DMLP_GDAXI'})\n",
        "\n",
        "Transformer_GDAXI_train = Transformer_GDAXI_train.rename(columns={'Prediction': 'Transformer_GDAXI'})\n",
        "Transformer_GDAXI_test = Transformer_GDAXI_test.rename(columns={'Prediction': 'Transformer_GDAXI'})\n",
        "\n",
        "Transformer_GSPC_train = Transformer_GSPC_train.rename(columns={'Prediction': 'Transformer_GSPC'})\n",
        "Transformer_GSPC_test = Transformer_GSPC_test.rename(columns={'Prediction': 'Transformer_GSPC'})\n",
        "\n",
        "Transformer_FTSE_train = Transformer_FTSE_train.rename(columns={'Prediction': 'Transformer_FTSE'})\n",
        "Transformer_FTSE_test = Transformer_FTSE_test.rename(columns={'Prediction': 'Transformer_FTSE'})\n",
        "\n",
        "Transformer_NDX_train = Transformer_NDX_train.rename(columns={'Prediction': 'Transformer_NDX'})\n",
        "Transformer_NDX_test = Transformer_NDX_test.rename(columns={'Prediction': 'Transformer_NDX'})\n",
        "\n",
        "Transformer_DJI_train = Transformer_DJI_train.rename(columns={'Prediction': 'Transformer_DJI'})\n",
        "Transformer_DJI_test = Transformer_DJI_test.rename(columns={'Prediction': 'Transformer_DJI'})\n",
        "\n",
        "XGB_DJI_train = XGB_DJI_train.rename(columns={'Prediction': 'XGB_DJI'})\n",
        "XGB_DJI_test = XGB_DJI_test.rename(columns={'Prediction': 'XGB_DJI'})\n",
        "\n",
        "XGB_GSPC_train = XGB_GSPC_train.rename(columns={'Prediction': 'XGB_GSPC'})\n",
        "XGB_GSPC_test = XGB_GSPC_test.rename(columns={'Prediction': 'XGB_GSPC'})\n",
        "\n",
        "XGB_FTSE_train = XGB_FTSE_train.rename(columns={'Prediction': 'XGB_FTSE'})\n",
        "XGB_FTSE_test = XGB_FTSE_test.rename(columns={'Prediction': 'XGB_FTSE'})\n",
        "\n",
        "XGB_NDX_train = XGB_NDX_train.rename(columns={'Prediction': 'XGB_NDX'})\n",
        "XGB_NDX_test = XGB_NDX_test.rename(columns={'Prediction': 'XGB_NDX'})\n",
        "\n",
        "XGB_GDAXI_train = XGB_GDAXI_train.rename(columns={'Prediction': 'XGB_GDAXI'})\n",
        "XGB_GDAXI_test = XGB_GDAXI_test.rename(columns={'Prediction': 'XGB_GDAXI'})\n",
        "\n",
        "# -------------------------\n",
        "# GSPC\n",
        "# -------------------------\n",
        "GSPC_train_preds = (\n",
        "    LSTM_GSPC_train[['LSTM_GSPC']]\n",
        "        .join(GRU_GSPC_train[['GRU_GSPC']], how='inner')\n",
        "        .join(DMLP_GSPC_train[['DMLP_GSPC']], how='inner')\n",
        "        .join(Transformer_GSPC_train[['Transformer_GSPC']], how='inner')\n",
        "        .join(XGB_GSPC_train[['XGB_GSPC']], how='inner')\n",
        ")\n",
        "\n",
        "GSPC_test_preds = (\n",
        "    LSTM_GSPC_test[['LSTM_GSPC']]\n",
        "        .join(GRU_GSPC_test[['GRU_GSPC']], how='inner')\n",
        "        .join(DMLP_GSPC_test[['DMLP_GSPC']], how='inner')\n",
        "        .join(Transformer_GSPC_test[['Transformer_GSPC']], how='inner')\n",
        "        .join(XGB_GSPC_test[['XGB_GSPC']], how='inner')\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# FTSE\n",
        "# -------------------------\n",
        "FTSE_train_preds = (\n",
        "    LSTM_FTSE_train[['LSTM_FTSE']]\n",
        "        .join(GRU_FTSE_train[['GRU_FTSE']], how='inner')\n",
        "        .join(DMLP_FTSE_train[['DMLP_FTSE']], how='inner')\n",
        "        .join(Transformer_FTSE_train[['Transformer_FTSE']], how='inner')\n",
        "        .join(XGB_FTSE_train[['XGB_FTSE']], how='inner')\n",
        ")\n",
        "\n",
        "FTSE_test_preds = (\n",
        "    LSTM_FTSE_test[['LSTM_FTSE']]\n",
        "        .join(GRU_FTSE_test[['GRU_FTSE']], how='inner')\n",
        "        .join(DMLP_FTSE_test[['DMLP_FTSE']], how='inner')\n",
        "        .join(Transformer_FTSE_test[['Transformer_FTSE']], how='inner')\n",
        "        .join(XGB_FTSE_test[['XGB_FTSE']], how='inner')\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# GDAXI\n",
        "# -------------------------\n",
        "GDAXI_train_preds = (\n",
        "    LSTM_GDAXI_train[['LSTM_GDAXI']]\n",
        "        .join(GRU_GDAXI_train[['GRU_GDAXI']], how='inner')\n",
        "        .join(DMLP_GDAXI_train[['DMLP_GDAXI']], how='inner')\n",
        "        .join(Transformer_GDAXI_train[['Transformer_GDAXI']], how='inner')\n",
        "        .join(XGB_GDAXI_train[['XGB_GDAXI']], how='inner')\n",
        ")\n",
        "\n",
        "GDAXI_test_preds = (\n",
        "    LSTM_GDAXI_test[['LSTM_GDAXI']]\n",
        "        .join(GRU_GDAXI_test[['GRU_GDAXI']], how='inner')\n",
        "        .join(DMLP_GDAXI_test[['DMLP_GDAXI']], how='inner')\n",
        "        .join(Transformer_GDAXI_test[['Transformer_GDAXI']], how='inner')\n",
        "        .join(XGB_GDAXI_test[['XGB_GDAXI']], how='inner')\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# DJI\n",
        "# -------------------------\n",
        "DJI_train_preds = (\n",
        "    LSTM_DJI_train[['LSTM_DJI']]\n",
        "        .join(GRU_DJI_train[['GRU_DJI']], how='inner')\n",
        "        .join(DMLP_DJI_train[['DMLP_DJI']], how='inner')\n",
        "        .join(Transformer_DJI_train[['Transformer_DJI']], how='inner')\n",
        "        .join(XGB_DJI_train[['XGB_DJI']], how='inner')\n",
        ")\n",
        "\n",
        "DJI_test_preds = (\n",
        "    LSTM_DJI_test[['LSTM_DJI']]\n",
        "        .join(GRU_DJI_test[['GRU_DJI']], how='inner')\n",
        "        .join(DMLP_DJI_test[['DMLP_DJI']], how='inner')\n",
        "        .join(Transformer_DJI_test[['Transformer_DJI']], how='inner')\n",
        "        .join(XGB_DJI_test[['XGB_DJI']], how='inner')\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# NDX\n",
        "# -------------------------\n",
        "NDX_train_preds = (\n",
        "    LSTM_NDX_train[['LSTM_NDX']]\n",
        "        .join(GRU_NDX_train[['GRU_NDX']], how='inner')\n",
        "        .join(DMLP_NDX_train[['DMLP_NDX']], how='inner')\n",
        "        .join(Transformer_NDX_train[['Transformer_NDX']], how='inner')\n",
        "        .join(XGB_NDX_train[['XGB_NDX']], how='inner')\n",
        ")\n",
        "\n",
        "NDX_test_preds = (\n",
        "    LSTM_NDX_test[['LSTM_NDX']]\n",
        "        .join(GRU_NDX_test[['GRU_NDX']], how='inner')\n",
        "        .join(DMLP_NDX_test[['DMLP_NDX']], how='inner')\n",
        "        .join(Transformer_NDX_test[['Transformer_NDX']], how='inner')\n",
        "        .join(XGB_NDX_test[['XGB_NDX']], how='inner')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slPTKh1IbafC"
      },
      "source": [
        "# Saving (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnls4nwkbdty"
      },
      "outputs": [],
      "source": [
        "dfs_to_save = {\n",
        "    \"GSPC_train_preds\": GSPC_train_preds,\n",
        "    \"GSPC_test_preds\": GSPC_test_preds,\n",
        "    \"FTSE_train_preds\": FTSE_train_preds,\n",
        "    \"FTSE_test_preds\": FTSE_test_preds,\n",
        "    \"GDAXI_train_preds\": GDAXI_train_preds,\n",
        "    \"GDAXI_test_preds\": GDAXI_test_preds,\n",
        "    \"DJI_train_preds\": DJI_train_preds,\n",
        "    \"DJI_test_preds\": DJI_test_preds,\n",
        "    \"NDX_train_preds\": NDX_train_preds,\n",
        "    \"NDX_test_preds\": NDX_test_preds\n",
        "}\n",
        "\n",
        "for df_name, df_obj in dfs_to_save.items():\n",
        "    file_path = os.path.join(f\"{df_name}.xlsx\")\n",
        "    df_obj.to_excel(file_path, index=True)\n",
        "    print(f\"Saved: {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ_iUcMPw1m-"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IykLCmonA8C5"
      },
      "outputs": [],
      "source": [
        "df_names = [\n",
        "    \"GSPC_train_preds\", \"GSPC_test_preds\",\n",
        "    \"FTSE_train_preds\", \"FTSE_test_preds\",\n",
        "    \"GDAXI_train_preds\", \"GDAXI_test_preds\",\n",
        "    \"DJI_train_preds\", \"DJI_test_preds\",\n",
        "    \"NDX_train_preds\", \"NDX_test_preds\"\n",
        "]\n",
        "\n",
        "dfs_loaded = {}\n",
        "\n",
        "for df_name in df_names:\n",
        "    file_path = os.path.join(f\"{df_name}.xlsx\")\n",
        "    if os.path.exists(file_path):\n",
        "        dfs_loaded[df_name] = pd.read_excel(file_path)\n",
        "        print(f\"Loaded: {file_path}\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "for df_name, df_obj in dfs_loaded.items():\n",
        "    globals()[df_name] = df_obj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeY6ZrJQmlOo"
      },
      "source": [
        "# Prepare data for financial calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqRLadF0vPsm"
      },
      "outputs": [],
      "source": [
        "START, END = \"2024-01-01\", \"2025-11-30\"\n",
        "tickers = [\"GSPC\", \"FTSE\", \"GDAXI\", \"DJI\", \"NDX\"]\n",
        "\n",
        "for t in tickers:\n",
        "    df = globals().get(f\"df_{t}\")\n",
        "    preds = globals().get(f\"{t}_test_preds\")\n",
        "    if df is None or preds is None:\n",
        "        print(f\"Skipping {t}: missing df_{t} or {t}_test_preds\")\n",
        "        continue\n",
        "\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    df['Close_yesterday'] = df['Close'].shift(1)\n",
        "    df[\"Change\"] = df[\"Close\"] / df[\"Close_yesterday\"] - 1\n",
        "\n",
        "    df_cut = df.loc[START:END]\n",
        "\n",
        "    preds[\"Date\"] = pd.to_datetime(preds[\"Date\"])\n",
        "    preds = preds.set_index(\"Date\")\n",
        "\n",
        "    preds = preds.merge(df_cut[[\"Close\", \"Close_yesterday\",\"Open\",\"High\",\"Low\",\"Change\"]], left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "    globals()[f\"df_{t}_cut\"] = df_cut\n",
        "    globals()[f\"{t}_test_preds\"] = preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9KIz_-dwQnp"
      },
      "outputs": [],
      "source": [
        "START, END = \"2021-01-01\", \"2024-01-01\"\n",
        "tickers = [\"GSPC\", \"FTSE\", \"GDAXI\", \"DJI\", \"NDX\"]\n",
        "\n",
        "for t in tickers:\n",
        "    df = globals().get(f\"df_{t}\")\n",
        "    preds = globals().get(f\"{t}_train_preds\")\n",
        "    if df is None or preds is None:\n",
        "        print(f\"Skipping {t}: missing df_{t} or {t}_train_preds\")\n",
        "        continue\n",
        "\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df_cut = df.loc[START:END]\n",
        "\n",
        "    preds[\"Date\"] = pd.to_datetime(preds[\"Date\"])\n",
        "    preds = preds.set_index(\"Date\")\n",
        "\n",
        "    preds = preds.merge(df_cut[[\"Close\", \"Close_yesterday\",\"Open\",\"High\",\"Low\",\"Change\"]], left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "    globals()[f\"df_{t}_cut\"] = df_cut\n",
        "    globals()[f\"{t}_train_preds\"] = preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble and Financial Analysis"
      ],
      "metadata": {
        "id": "9LB5T_duxotv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_change_columns(df):\n",
        "    suffix = next(c.rsplit('_', 1)[1] for c in df.columns if '_' in c)\n",
        "    cols = [c for c in df.columns if c.endswith(suffix)]\n",
        "    for c in cols:\n",
        "        model = c[:-(len(suffix)+1)]\n",
        "        df[f\"change_{model}_{suffix}\"] = df[c].div(df[c].shift(1)).sub(1)\n",
        "    return df\n",
        "\n",
        "names = [\n",
        "    \"DJI_train_preds\",\"DJI_test_preds\",\n",
        "    \"FTSE_train_preds\",\"FTSE_test_preds\",\n",
        "    \"GDAXI_train_preds\",\"GDAXI_test_preds\",\n",
        "    \"GSPC_train_preds\",\"GSPC_test_preds\",\n",
        "    \"NDX_train_preds\",\"NDX_test_preds\",\n",
        "]\n",
        "\n",
        "for name in names:\n",
        "    df = add_change_columns(locals()[name])\n",
        "\n",
        "    if \"Date\" in df.columns and df.index.name != \"Date\":\n",
        "        df = df.set_index(\"Date\")\n",
        "\n",
        "    df = df.iloc[1:]\n",
        "    df = df.sort_index()\n",
        "\n",
        "    locals()[name] = df\n"
      ],
      "metadata": {
        "id": "qo2s6zCKpKxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODELS = [\"LSTM\", \"GRU\", \"DMLP\", \"Transformer\", \"XGB\"]\n",
        "tickers = [\"DJI\", \"FTSE\", \"GDAXI\", \"GSPC\", \"NDX\"]\n",
        "\n",
        "fee = 0.0001\n",
        "plotting = 0\n",
        "\n",
        "names = [\n",
        "    \"DJI_train_preds\",\"DJI_test_preds\",\n",
        "    \"FTSE_train_preds\",\"FTSE_test_preds\",\n",
        "    \"GDAXI_train_preds\",\"GDAXI_test_preds\",\n",
        "    \"GSPC_train_preds\",\"GSPC_test_preds\",\n",
        "    \"NDX_train_preds\",\"NDX_test_preds\",\n",
        "]\n",
        "\n",
        "dataframes_by_name = {name: globals()[name] for name in names}\n",
        "\n",
        "results = []\n",
        "results_b = []\n",
        "\n",
        "def subsets_min_k(items, k: int):\n",
        "    out = []\n",
        "    for r in range(k, len(items)+1):\n",
        "        for comb in itertools.combinations(items, r):\n",
        "            out.append(tuple(sorted(comb)))\n",
        "    return out\n",
        "\n",
        "SUBSETS_2PLUS = subsets_min_k(BASE_MODELS, 2)\n",
        "\n",
        "def run_name(ticker, model_type, subset):\n",
        "    return f\"{ticker}_{model_type}_{'_'.join(subset)}\"\n",
        "\n",
        "def price_cols(ticker, subset):\n",
        "    return [f\"{m}_{ticker}\" for m in subset]\n",
        "\n",
        "def change_cols(ticker, subset):\n",
        "    return [f\"change_{m}_{ticker}\" for m in subset]\n",
        "\n",
        "def add_financials(df_test, run_col, ticker, model_type):\n",
        "    a, b = financials(df_test, run_col, ticker, model_type, fee, plotting=plotting)\n",
        "    a = a.copy()\n",
        "    a[\"ticker\"] = ticker\n",
        "    a[\"model\"] = model_type\n",
        "    a[\"run\"] = run_col\n",
        "    results.append(a)\n",
        "    results_b.append(b)\n",
        "\n",
        "def pred_close_from_change(df, pred_change):\n",
        "    cy = pd.to_numeric(df[\"Close_yesterday\"], errors=\"coerce\").to_numpy(float)\n",
        "    return cy * (1.0 + pred_change)"
      ],
      "metadata": {
        "id": "mUlhHxEfpHpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ticker in tickers:\n",
        "    tr = dataframes_by_name[f\"{ticker}_train_preds\"]\n",
        "    te = dataframes_by_name[f\"{ticker}_test_preds\"]\n",
        "\n",
        "    for subset in SUBSETS_2PLUS:\n",
        "        cols = price_cols(ticker, subset)\n",
        "\n",
        "        avg_tr = tr[cols].apply(pd.to_numeric, errors=\"coerce\").mean(axis=1)\n",
        "        avg_te = te[cols].apply(pd.to_numeric, errors=\"coerce\").mean(axis=1)\n",
        "\n",
        "        colname = run_name(ticker, \"AVG\", subset)\n",
        "\n",
        "        if colname not in tr.columns: tr[colname] = avg_tr\n",
        "        if colname not in te.columns: te[colname] = avg_te\n",
        "\n",
        "        add_financials(te, colname, ticker, \"AVG\")\n",
        "\n",
        "print(\"AVG done.\")\n"
      ],
      "metadata": {
        "id": "jE6fsaWqpJx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ticker in tickers:\n",
        "    te = dataframes_by_name[f\"{ticker}_test_preds\"]\n",
        "\n",
        "    for m in BASE_MODELS:\n",
        "        colname = f\"{m}_{ticker}\"   # existing column\n",
        "        add_financials(te, colname, ticker, m)\n",
        "\n",
        "print(\"Base singles done.\")\n"
      ],
      "metadata": {
        "id": "WSdnYGOwpT4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "     (\"lr\", LinearRegression())])\n",
        "\n",
        "for ticker in tickers:\n",
        "    tr = dataframes_by_name[f\"{ticker}_train_preds\"]\n",
        "    te = dataframes_by_name[f\"{ticker}_test_preds\"]\n",
        "\n",
        "    y = pd.to_numeric(tr[\"Change\"], errors=\"coerce\").to_numpy(float)\n",
        "\n",
        "    for subset in SUBSETS_2PLUS:\n",
        "        cols = change_cols(ticker, subset)\n",
        "\n",
        "        Xtr = tr[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(float)\n",
        "        Xte = te[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(float)\n",
        "\n",
        "        model.fit(Xtr, y)\n",
        "\n",
        "        pred_te_change = model.predict(Xte)\n",
        "        pred_te_close  = pred_close_from_change(te, pred_te_change)\n",
        "\n",
        "        colname = run_name(ticker, \"LR\", subset)\n",
        "        if colname not in te.columns: te[colname] = pred_te_close\n",
        "\n",
        "        add_financials(te, colname, ticker, \"LR\")\n",
        "\n",
        "print(\"LR done.\")\n"
      ],
      "metadata": {
        "id": "gXqGoGJUpYQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"ridge\", Ridge(alpha=1.0, random_state=0))\n",
        "])\n",
        "\n",
        "for ticker in tickers:\n",
        "    tr = dataframes_by_name[f\"{ticker}_train_preds\"]\n",
        "    te = dataframes_by_name[f\"{ticker}_test_preds\"]\n",
        "\n",
        "    y = pd.to_numeric(tr[\"Change\"], errors=\"coerce\").to_numpy(float)\n",
        "\n",
        "    for subset in SUBSETS_2PLUS:\n",
        "        cols = change_cols(ticker, subset)\n",
        "\n",
        "        Xtr = tr[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(float)\n",
        "        Xte = te[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(float)\n",
        "\n",
        "        model.fit(Xtr, y)\n",
        "\n",
        "        pred_te_close = pred_close_from_change(te, model.predict(Xte))\n",
        "\n",
        "        colname = run_name(ticker, \"PR\", subset)\n",
        "        if colname not in te.columns: te[colname] = pred_te_close\n",
        "\n",
        "        add_financials(te, colname, ticker, \"PR\")\n",
        "\n",
        "print(\"PR done.\")\n"
      ],
      "metadata": {
        "id": "8ez14M7TpeHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"en\", ElasticNet(alpha=1e-3, l1_ratio=0.5, max_iter=20000, random_state=0))\n",
        "])\n",
        "\n",
        "for ticker in tickers:\n",
        "    tr = dataframes_by_name[f\"{ticker}_train_preds\"]\n",
        "    te = dataframes_by_name[f\"{ticker}_test_preds\"]\n",
        "\n",
        "    y = pd.to_numeric(tr[\"Change\"], errors=\"coerce\").to_numpy(float)\n",
        "\n",
        "    for subset in SUBSETS_2PLUS:\n",
        "        cols = change_cols(ticker, subset)\n",
        "\n",
        "        Xtr = tr[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(float)\n",
        "        Xte = te[cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(float)\n",
        "\n",
        "        model.fit(Xtr, y)\n",
        "\n",
        "        pred_te_close = pred_close_from_change(te, model.predict(Xte))\n",
        "\n",
        "        colname = run_name(ticker, \"EN\", subset)\n",
        "        if colname not in te.columns: te[colname] = pred_te_close\n",
        "\n",
        "        add_financials(te, colname, ticker, \"EN\")\n",
        "\n",
        "final_results_df = pd.concat(results, ignore_index=True) if results else pd.DataFrame()"
      ],
      "metadata": {
        "id": "nWNS1yWQpgri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving (if needed)"
      ],
      "metadata": {
        "id": "9WVrpsRfyXDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_results_df.to_excel(\"final_results_df_new.xlsx\", index=True)"
      ],
      "metadata": {
        "id": "3jP4XXz_kXCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBZGvn5onJjF"
      },
      "source": [
        "## Saving final results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_results_df['R_squered'] = final_results_df['R_squered'].astype(str).str.replace(\",\", \".\").astype(float)\n",
        "final_results_df['Total Return'] = final_results_df['Total Return'].astype(str).str.replace(\",\", \".\").astype(float)\n",
        "\n",
        "pivot_r2 = final_results_df.pivot_table(index=\"ticker\", columns=\"Algorithm\", values=\"R_squered\", aggfunc=\"mean\")\n",
        "pivot_return = final_results_df.pivot_table(index=\"ticker\", columns=\"Algorithm\", values=\"Total Return\", aggfunc=\"mean\")\n",
        "\n",
        "# Plot heatmap for R^2\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(pivot_r2, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", cbar_kws={'label': 'R'})\n",
        "plt.title(\"Model performance by index (R)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"heatmap_r2.pdf\")\n",
        "plt.savefig(\"heatmap_r2.png\", dpi=300)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot heatmap for Total Return\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(pivot_return, annot=True, fmt=\".2%\", cmap=\"RdYlGn\", center=0, cbar_kws={'label': 'Total Return'})\n",
        "plt.title(\"Model performance by index (Total Return)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"heatmap_return.pdf\")\n",
        "plt.savefig(\"heatmap_return.png\", dpi=300)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "mxIH1bzVwLcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpKqh9cZnSzm"
      },
      "source": [
        "## Loading final results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJ8iaGGxGp5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"final_results_df.xlsx\", sheet_name=\"Sheet1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7SS8up_eMsC"
      },
      "source": [
        "## scatter plot of R2 vs. Total Return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[\"R_squered\"].values\n",
        "y = df[\"Total Return\"].values * 100\n",
        "\n",
        "# Pearson correlation\n",
        "r, _ = pearsonr(x, y)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "hb = ax.hexbin(\n",
        "    x, y,\n",
        "    gridsize=40,\n",
        "    mincnt=1,\n",
        "    cmap=\"viridis\"\n",
        ")\n",
        "\n",
        "# Regression line\n",
        "m, b = np.polyfit(x, y, 1)\n",
        "xx = np.linspace(x.min(), x.max(), 100)\n",
        "ax.plot(xx, m * xx + b, color=\"tab:blue\", linewidth=2)\n",
        "\n",
        "ax.set_xlabel(r\"$R^2$\")\n",
        "ax.set_ylabel(\"Total Return (%)\")\n",
        "ax.set_title(rf\"$R^2$ vs Total Return (Pearson r = {r:.2f})\")\n",
        "\n",
        "cb = fig.colorbar(hb, ax=ax)\n",
        "cb.set_label(\"Point density\")\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"r2_vs_total_return.pdf\")\n",
        "plt.show()\n",
        "plt.close(fig)\n"
      ],
      "metadata": {
        "id": "xBIXc3lfOA4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndv0eGfinpEv"
      },
      "source": [
        "## heatmap for R^2 and heatmap for Total Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2CnpQiXiDjz"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['R_squered'] = df['R_squered'].astype(str).str.replace(\",\", \".\").astype(float)\n",
        "df['Total Return'] = df['Total Return'].astype(str).str.replace(\",\", \".\").astype(float)\n",
        "\n",
        "pivot_r2 = df.pivot_table(index=\"ticker\", columns=\"Algorithm\", values=\"R_squered\", aggfunc=\"mean\")\n",
        "pivot_return = df.pivot_table(index=\"ticker\", columns=\"Algorithm\", values=\"Total Return\", aggfunc=\"mean\")\n",
        "\n",
        "# Plot heatmap for R^2\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(pivot_r2, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", cbar_kws={'label': 'R'})\n",
        "plt.title(\"Model performance by index (R)\")\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"heatmap_r2.pdf\")\n",
        "# plt.savefig(\"heatmap_r2.png\", dpi=300)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot heatmap for Total Return\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(pivot_return, annot=True, fmt=\".2%\", cmap=\"RdYlGn\", center=0, cbar_kws={'label': 'Total Return'})\n",
        "plt.title(\"Model performance by index (Total Return)\")\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"heatmap_return.pdf\")\n",
        "# plt.savefig(\"heatmap_return.png\", dpi=300)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}